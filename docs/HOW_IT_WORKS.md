# How the Intelligent Table Picker Works

A complete guide in simple, everyday language

---

## The Problem We're Solving

Imagine you have a large database with many tables. A user asks a question in plain English like:

**"Show me all students who scored above 80"**

The computer needs to figure out:
1. Which tables have the information needed?
2. Which tables are most relevant?
3. Should it look at 1 table or multiple tables?

This is surprisingly difficult because:
- Databases can have dozens or hundreds of tables
- Table names might not match what users say
- Users might need data from multiple related tables
- The same information could exist in different places

**Our job**: Build a smart system that automatically picks the right tables.

---

## The Big Picture: How It Works

Think of our system like a librarian who helps you find the right books in a huge library:

1. **You ask a question** (in normal language)
2. **The librarian understands** what you're looking for
3. **The librarian searches** through the catalog
4. **The librarian scores each book** by how relevant it is
5. **The librarian recommends** the top 3-5 most relevant books

Our system does the same thing, but with database tables instead of books!

---

## Step-by-Step: The Complete Process

### Step 1: Understanding What You're Asking For

When you type: **"Show me all students"**

The system first **extracts the important words**:
- It removes filler words like "show", "me", "all" (these don't help)
- It keeps meaningful words: "students"
- It converts everything to lowercase for consistency

**Example:**
```
Input:  "Show me all students who live in hostel H1"
Output: ["students", "hostel"]
```

**Words that get removed** (called "stopwords"):
- Common verbs: show, get, find, list, display
- Articles: the, a, an
- Prepositions: of, for, in, on, at
- Other filler words that don't identify tables or data

---

### Step 2: The Knowledge Graph - Our Secret Weapon

Before we can pick tables, we need to understand what's in our database. We build something called a **Knowledge Graph** - think of it as a detailed map of the entire database.

**What the Knowledge Graph contains:**

1. **Basic Info**
   - Table names
   - Number of rows in each table
   - Column names and types

2. **Sample Data**
   - What values actually appear in each column
   - Most common values (like "Computer Science", "2023")

3. **Relationships**
   - Which tables connect to each other
   - How tables are related (Foreign Keys)

4. **Smart Hints**
   - Which columns are good for filtering
   - Which columns are good for grouping
   - Which columns are good for calculations

5. **Synonyms** (user-defined)
   - Alternative words users might use
   - Example: "learner" means "student", "teacher" means "instructor"

**Analogy**: It's like having a detailed guidebook for a city that tells you:
- What's at each location
- How locations connect to each other
- What you can do at each location
- Alternative names people use for locations

---

### Step 3: Scoring Every Table

Now comes the magic! For each table in the database, we calculate a **relevance score** - a number that tells us how well this table matches what the user is asking for.

**Think of it like scoring resumes for a job**:
- Perfect qualification = more points
- Partial match = some points
- No match = zero points

We use **8 different scoring methods** to evaluate each table:

---

#### Scoring Method #1: Table Name Match (10 points) ðŸ†

**What it does:** Checks if the query words appear in the table name.

**Why it matters:** If someone asks for "students", a table called "students_info" is probably what they want!

**Example:**
```
Query: "Show me all courses"

Table: courses
Score: +10 points (table name contains "courses")

Table: faculty_info
Score: 0 points (doesn't contain "courses")
```

**Real-world analogy:** If you ask for "mystery books", a shelf labeled "Mystery" gets the highest score.

---

#### Scoring Method #2: Synonym Match (7 points) ðŸŽ¯

**What it does:** Checks if query words match user-defined synonyms.

**Why it matters:** Users might use different words than what's in the database. "Learner" means "student", "teacher" means "instructor".

**Example:**
```
Query: "Find all learners in batch 2023"

You've defined: learner = student (in your synonym file)

Table: students_info
  Column: Student ID
  Synonym: "learner"
  Score: +7 points (synonym matches!)
```

**Real-world analogy:** You ask for "fizzy drinks" and the store clerk knows you mean "soda" - they help you find the soda section.

---

#### Scoring Method #3: Column Name Match (5 points) ðŸ“‹

**What it does:** Checks if query words appear in column names.

**Why it matters:** If you ask for "email addresses", a table with an "email" column probably has what you need.

**Example:**
```
Query: "Show student email addresses"

Table: students_info
  Columns: Name, Email, Phone
  Score: +5 points (column "Email" matches "email")
         +5 points (column name matches "student")
  Total: 10 points from column matches
```

**Real-world analogy:** Looking for "contact information" and finding a file cabinet drawer labeled "Contact Info".

---

#### Scoring Method #4: Foreign Key Relationship (4 points) ðŸ”—

**What it does:** Boosts scores for tables that connect to other high-scoring tables.

**Why it matters:** Related tables are often needed together. If you need student data, you probably also need their enrollment data.

**How it works:**
1. Score all tables first
2. Find the top 3 highest-scoring tables
3. Look for other tables connected to these top 3
4. Boost those connected tables

**Example:**
```
Query: "Show student grades"

Initial Scores:
  students_info: 10 pts (name matches)
  grades: 10 pts (name matches)
  courses: 5 pts (related)

FK Boost Phase:
  - grades connects to students_info â†’ +4 pts
  - grades also connects to courses â†’ +4 pts

Final Score for grades: 18 pts (best overall!)
```

**Real-world analogy:** You're researching a historical figure. You find their biography (high relevance), and the librarian also recommends their letters and diary (related documents) because they're connected.

---

#### Scoring Method #5: Semantic Type Match (3 points) ðŸ§©

**What it does:** Matches the "type" of data you need with columns that have that type.

**Column types we detect:**
- **TEMPORAL**: Dates and times
- **NUMERICAL**: Numbers (for math operations)
- **CATEGORICAL**: Categories or groups
- **TEXT**: Text descriptions
- **IDENTIFIER**: IDs and keys

**How it works:**
The system reads your query to understand your intent:

- Query mentions "date", "when", "year" â†’ looks for TEMPORAL columns
- Query mentions "average", "total", "sum" â†’ looks for NUMERICAL columns
- Query mentions "group", "category", "type" â†’ looks for CATEGORICAL columns

**Example:**
```
Query: "What's the average grade per course?"
       ^^^^^^^ (needs math operation)
                            ^^^^ (needs grouping)

Table: grades
  Column: Grade (NUMERICAL type) â†’ +3 pts

Table: courses
  Column: Course Name (CATEGORICAL) â†’ +3 pts
```

**Real-world analogy:** You ask for "action movies" and the system knows to look in the genre (category) field, not the title field.

---

#### Scoring Method #6: Query Hints (3 points) ðŸ’¡

**What it does:** Matches your query operation with columns optimized for that operation.

**Query operations we detect:**
- **Filtering** (WHERE clause): "only", "where", "filter"
- **Grouping** (GROUP BY): "group", "per", "each", "by"
- **Aggregation** (SUM/AVG/COUNT): "total", "average", "count"

**How columns get hints:**
When building the Knowledge Graph, we analyze each column:
- High cardinality (many unique values) â†’ good for filtering
- Low cardinality (few unique values) â†’ good for grouping
- Numerical columns â†’ good for aggregation

**Example:**
```
Query: "Count students per department"
       ^^^^^ (aggregation)   ^^^^^^^^^ (grouping)

Table: students_info
  Column: Department (marked as "good for grouping")
  Score: +3 points
```

**Real-world analogy:** You want to organize books by genre. The system knows the "Genre" field is perfect for organizing (grouping), not the "Page Count" field.

---

#### Scoring Method #7: Sample Value Match (2 points) ðŸ“Š

**What it does:** Checks if words in your query appear in the actual data stored in the table.

**Why it matters:** If you mention "Computer Science" and that exact phrase appears in a column's data, that table probably has what you need.

**Example:**
```
Query: "Find students in Computer Science"

Table: students_info
  Column: Department
  Sample values: ["Computer Science", "Mechanical", "Civil"]
  Score: +2 points ("Computer Science" found in samples!)
```

**Real-world analogy:** You're looking for documents about "Project Apollo" and find a folder where that exact phrase appears in the file contents.

---

#### Scoring Method #8: Top Value Match (2 points) â­

**What it does:** Similar to sample values, but focuses on the most common/frequent values.

**Why it matters:** Common values are often important domain terms. If "Active" is the most common status, and you ask for "active students", that's a strong signal.

**Example:**
```
Query: "Show active students"

Table: students_info
  Column: Status
  Top values: ["Active", "Graduated", "Inactive"]
  Score: +2 points ("Active" is a top value!)
```

**Real-world analogy:** You ask for "bestselling books" and the librarian knows which ones are most popular because they track what sells most.

---

### Step 4: Filtering - Deciding How Many Tables to Recommend

After scoring every table, we don't want to recommend too many or too few. We use an **adaptive filtering strategy** that adjusts based on the situation.

**Think of it like a restaurant menu**:
- Too many options = overwhelming
- Too few options = limiting
- Just right = helpful

**Our filtering rules:**

#### Rule 1: Absolute Threshold (5 points minimum)
**"Only recommend tables that are at least somewhat relevant"**

- Keep all tables with score â‰¥ 5 points
- Filters out tables with very low relevance

**Example:**
```
Scores after Step 3:
  students_info: 15 pts âœ“ Keep
  grades: 12 pts âœ“ Keep
  courses: 8 pts âœ“ Keep
  hostel: 6 pts âœ“ Keep
  faculty_info: 3 pts âœ— Remove (below threshold)
```

#### Rule 2: Relative Threshold (30% of top score)
**"If too many tables passed, be more selective"**

- If more than 8 tables passed Rule 1
- Use 30% of the highest score as the new threshold
- But never go below 5 points

**Example:**
```
Top score: 20 pts
30% of 20 = 6 pts

Keep only tables with score â‰¥ 6 pts
```

#### Rule 3: Minimum Fallback (5 tables)
**"Always give at least a few options, even for vague queries"**

- If fewer than 2 tables passed
- Take the top 5 tables anyway
- Prevents "no results" for vague queries

**Example:**
```
Query: "Show me data"  (very vague!)

Scores:
  All tables: 0-2 pts (nothing scored well)

Fallback: Return top 5 tables anyway
```

#### Rule 4: Maximum Cap (8 tables)
**"Never overwhelm with too many options"**

- Cap at 8 tables maximum
- Keeps token usage manageable for the LLM
- Prevents confusion from too many choices

---

### Step 5: FK Relationship Boosting - Finding the Missing Pieces

After filtering, we do one more smart thing: **look for tables that connect the pieces together**.

**The Problem:**
Sometimes junction tables (middle tables that connect others) score low initially, but are actually essential.

**Example Scenario:**
```
Query: "Which students are in which courses?"

After filtering:
  1. students_info (10 pts) âœ“
  2. courses (10 pts) âœ“
  3. enrollments (3 pts) - PROBLEM! Too low!

But "enrollments" is the junction table that connects students to courses!
Without it, you can't answer the query.
```

**The Solution:**
We check which tables connect to our top candidates and boost them.

**How it works:**

1. Take the top 3 candidates from filtering
2. Look for tables that have Foreign Keys to these top 3
3. Boost those connecting tables (+4 pts per connection)
4. Tables connecting to multiple top candidates get boosted even more!

**Example:**
```
After Filtering:
  1. students_info: 10 pts
  2. courses: 10 pts
  3. grades: 5 pts

FK Boost Check:
  - enrollments has FK to students_info â†’ +4 pts
  - enrollments has FK to courses â†’ +4 pts

  enrollments new score: 3 + 4 + 4 = 11 pts

Final Ranking:
  1. enrollments: 11 pts â† Now it's at the top!
  2. students_info: 10 pts
  3. courses: 10 pts
```

**Real-world analogy:** You're planning a trip to Paris. You found flights and hotels (high scores), but the system also recommends the "Connecting Flights" folder because it links your city to Paris - even though you didn't explicitly search for it.

---

## Complete Example: End-to-End

Let's walk through a complete example to see all the pieces working together.

**User Query:** "What's the average grade for students in Computer Science?"

---

### Phase 1: Extract Keywords

```
Input: "What's the average grade for students in Computer Science?"

Removed words: "what's", "the", "for", "in"

Extracted keywords: ["average", "grade", "students", "computer", "science"]
```

---

### Phase 2: Score All Tables

**Table: students_info**
```
âœ“ Table Name Match: contains "students" â†’ +10 pts
âœ“ Column Name Match: "Student ID" contains "student" â†’ +5 pts
âœ“ Sample Value Match: "Computer Science" in Department â†’ +2 pts

Total: 17 points
```

**Table: grades**
```
âœ“ Table Name Match: contains "grade" â†’ +10 pts
âœ“ Column Name Match: "Student ID" contains "student" â†’ +5 pts
âœ“ Column Name Match: "Grade" contains "grade" â†’ +5 pts
âœ“ Semantic Type: "Grade" is NUMERICAL (query needs average) â†’ +3 pts
âœ“ Query Hint: "Grade" is good for aggregation â†’ +3 pts

Total: 26 points
```

**Table: courses**
```
âœ“ Column Name Match: "Course Code" contains "course" â†’ +5 pts
âœ“ Sample Value Match: "Computer Science" in Department â†’ +2 pts

Total: 7 points
```

**Table: enrollments**
```
âœ“ Column Name Match: "Grade" contains "grade" â†’ +5 pts

Total: 5 points
```

**Table: faculty_info**
```
âœ— No matches

Total: 0 points
```

**Table: hostel**
```
âœ— No matches

Total: 0 points
```

---

### Phase 3: Filter by Threshold

**Absolute Threshold = 5 pts**

```
Before filtering:
  grades: 26 pts
  students_info: 17 pts
  courses: 7 pts
  enrollments: 5 pts
  faculty_info: 0 pts âœ— Below threshold
  hostel: 0 pts âœ— Below threshold

After filtering: 4 tables remain
```

**Check: Do we have too many (>8)?** No, we have 4. âœ“
**Check: Do we have too few (<2)?** No, we have 4. âœ“

Filtering complete!

---

### Phase 4: FK Relationship Boost

**Top 3 candidates:**
1. grades (26 pts)
2. students_info (17 pts)
3. courses (7 pts)

**Check for connections:**

```
enrollments:
  - Has FK to students_info â†’ +4 pts
  - Has FK to courses â†’ +4 pts

  New score: 5 + 4 + 4 = 13 pts

courses:
  - Has FK to grades â†’ +4 pts

  New score: 7 + 4 = 11 pts
```

**Re-sort after boosting:**

```
Final Ranking:
  1. grades: 26 pts
  2. students_info: 17 pts
  3. enrollments: 13 pts â† Boosted up!
  4. courses: 11 pts
```

---

### Phase 5: Results Returned

**Top Candidates (with explanations):**

**1. grades (26 points)**
- âœ“ Table name matches "grade"
- âœ“ Has "Grade" column (numerical, good for averaging)
- âœ“ Has "Student ID" column
- âœ“ Optimized for aggregation operations

**2. students_info (17 points)**
- âœ“ Table name matches "students"
- âœ“ Has "Computer Science" in data
- âœ“ Contains student information

**3. enrollments (13 points)**
- âœ“ Connects students to courses
- âœ“ Essential junction table
- âœ“ Boosted due to FK relationships

**4. courses (11 points)**
- âœ“ Contains course information
- âœ“ Connected via FK

---

## Why This Approach is Powerful

### 1. **Multi-Signal Scoring**
We don't rely on just one clue. We combine:
- Names (tables and columns)
- Data content (sample values)
- Structure (relationships)
- Semantics (types and operations)
- User knowledge (synonyms)

**Analogy:** Like a detective using fingerprints, witnesses, video footage, and forensics together - multiple clues are better than one.

---

### 2. **Adaptive Filtering**
The system adjusts based on the situation:
- Clear queries (high scores) â†’ fewer tables
- Vague queries (low scores) â†’ more options
- Complex queries â†’ up to 8 tables

**Analogy:** Like a good teacher who gives more guidance when students are confused, less when they're confident.

---

### 3. **Relationship-Aware**
Doesn't just look at tables in isolation - understands how they connect.

**Analogy:** Like planning a road trip - it's not just about the destinations, but also the roads connecting them.

---

### 4. **User-Customizable**
You can add synonyms to teach the system your domain language.

**Analogy:** Like teaching a assistant your company's specific terminology and jargon.

---

### 5. **Explainable**
Every score has reasons. You can see exactly why each table was chosen.

**Analogy:** Like a doctor explaining their diagnosis step-by-step, not just giving you a prescription.

---

## Real-World Performance

From our tests on an education database:

**Accuracy: 100% (12 out of 12 queries)**

âœ“ Simple queries: "Show all students" â†’ students_info
âœ“ Multi-table queries: "Student grades" â†’ students_info + grades
âœ“ Complex queries: "Students with hostel info" â†’ students_info + hostel
âœ“ Specific queries: "Fee due information" â†’ feedue

**Speed:**
- Scoring all tables: < 50ms
- Complete pipeline: < 100ms
- Fast enough for real-time use

---

## What Makes Each Scoring Method Useful

Let's recap when each method shines:

| Method | Best For | Example |
|--------|----------|---------|
| **Table Name** | Direct table requests | "Show students" |
| **Synonym** | Domain terminology | "Find learners" (learner=student) |
| **Column Name** | Attribute queries | "Get email addresses" |
| **FK Relationship** | Multi-table queries | "Students enrolled in courses" |
| **Semantic Type** | Analytical queries | "Average grade per department" |
| **Query Hints** | Operation-specific | "Count students by batch" |
| **Sample Values** | Specific data points | "Students in Computer Science" |
| **Top Values** | Common categories | "Show active students" |

---

## Common Patterns and Examples

### Pattern 1: Simple Lookup
**"Show me all courses"**

- Table name match dominates (10 pts)
- Clear winner: courses table
- Result: 1 table recommended

---

### Pattern 2: Attribute Search
**"Find student email addresses"**

- Table name match: students_info (10 pts)
- Column name match: Email column (5 pts)
- Result: students_info (15 pts total)

---

### Pattern 3: Multi-Table Query
**"Which students are enrolled in which courses"**

- students_info: name match (10 pts)
- courses: name match (10 pts)
- enrollments: FK boost (connects both) (8 pts)
- Result: All 3 tables recommended

---

### Pattern 4: Analytical Query
**"What's the average grade by department?"**

- grades: numerical column + aggregation hint (16 pts)
- students_info: department column + grouping hint (13 pts)
- Result: Both tables recommended

---

### Pattern 5: Specific Data Query
**"Students in Computer Science department"**

- students_info: table match (10 pts) + sample value match (2 pts)
- Result: students_info (12 pts)

---

### Pattern 6: Vague Query
**"Show me data"**

- No good matches (all < 5 pts)
- Fallback rule triggers
- Result: Top 5 tables returned

---

## Tips for Best Results

### For Users:

1. **Be specific** - "student grades" is better than "information"
2. **Use actual terms** - use words that appear in table/column names
3. **Mention relationships** - "students enrolled in courses" hints at junction tables
4. **Include operations** - "average grade" helps identify numerical columns

### For Database Designers:

1. **Use clear table names** - "students_info" is better than "tbl_001"
2. **Use descriptive column names** - "Email" is better than "col_5"
3. **Define FK relationships** - helps the system understand connections
4. **Add synonyms** - teach the system domain-specific terminology

### For System Maintainers:

1. **Update synonyms regularly** - as terminology evolves
2. **Monitor failed queries** - identify gaps in coverage
3. **Tune weights if needed** - based on actual usage patterns
4. **Keep KG updated** - rebuild when schema changes

---

## Limitations and When It Might Struggle

### 1. **Completely Vague Queries**
**Query:** "Give me data"

**Problem:** No specific terms to match
**Result:** Falls back to top 5 tables (might not be what you want)
**Solution:** Add more specific terms

---

### 2. **Unusual Terminology**
**Query:** "Show me learners" (when table is "students" and no synonym defined)

**Problem:** No exact match, no synonym
**Result:** Might miss the right table
**Solution:** Add "learner" as synonym for "student"

---

### 3. **Implicit Requirements**
**Query:** "Show student names"

**Problem:** User might also need ID, contact info, etc.
**Result:** Only returns students table (not grades, enrollments)
**Solution:** System only finds what you explicitly ask for

---

### 4. **Ambiguous Terms**
**Query:** "Show information"

**Problem:** Many tables might contain "info" or "information"
**Result:** Multiple tables score similarly
**Solution:** Be more specific about what kind of information

---

### 5. **Missing Metadata**
**Query:** "Students from New York"

**Problem:** If "New York" isn't in sample values
**Result:** Won't get sample value bonus
**Solution:** Ensure KG has good sample data

---

## Future Enhancements (Phase 2)

What could make this even better:

### 1. **Semantic Similarity (Automatic Synonyms)**
Instead of manually defining "learner" = "student", use AI embeddings to understand that words are similar automatically.

**Example:** "revenue" would automatically match "sales" without manual configuration.

---

### 2. **Query Intent Understanding**
Use LLM to understand the deeper meaning of queries.

**Example:**
**Query:** "Who are the top performers?"
**Intent:** Ranking query needing grades + student info + ordering

---

### 3. **Learning from Feedback**
Track which tables users actually use after recommendations and improve over time.

**Example:** If users always skip recommendation #3, lower its weight.

---

### 4. **Multi-Language Support**
Handle queries in different languages.

---

### 5. **Context Awareness**
Remember previous queries in a conversation.

**Example:**
**User:** "Show me courses"
**System:** [Returns courses]
**User:** "Who teaches them?"
**System:** [Remembers "them" = courses]

---

## Summary: The Complete Flow

```
User Query
    â†“
[Extract Keywords]
Remove stopwords, clean text
    â†“
[Score All Tables]
Apply 8 scoring methods to each table
    â†“
[Filter by Threshold]
Keep only relevant tables (adaptive rules)
    â†“
[FK Relationship Boost]
Boost connected tables
    â†“
[Rank & Return]
Top 3-8 tables with explanations
```

**Key Strengths:**
- âœ“ Fast (< 100ms)
- âœ“ Accurate (100% in tests)
- âœ“ Explainable (shows reasoning)
- âœ“ Adaptive (adjusts to query complexity)
- âœ“ Relationship-aware (finds junction tables)
- âœ“ Customizable (user-defined synonyms)

**The Result:**
A smart system that reliably picks the right database tables, making it much easier to convert natural language questions into database queries.

---

## Glossary

**Knowledge Graph**: A detailed map of your database containing table structures, relationships, sample data, and metadata.

**Scoring**: Calculating how relevant each table is to the query by checking multiple factors.

**Foreign Key (FK)**: A column that references another table, creating a relationship between them.

**Junction Table**: A table that connects two other tables in a many-to-many relationship.

**Semantic Type**: The "meaning" of a column's data (date, number, category, etc.)

**Threshold**: A minimum score required for a table to be recommended.

**Synonym**: An alternative word that means the same thing (e.g., "learner" for "student").

**Stopword**: Common words that don't help identify tables (like "the", "show", "get").

**Cardinality**: How many unique values a column has (high = many unique, low = few unique).

---

*Document created for the KG-Enhanced Table Picker project*
*Last updated: January 2026*
